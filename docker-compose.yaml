  
services:
  postgres:
    image: postgres:15
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"  # be careful of port assignment, can be problem when you have already postgres

  airflow:
    build:
      context: .
      dockerfile: Docker.airflow
    image: my-airflow:2.8.1
    container_name: airflow_webserver
    user: root
    depends_on:
        - postgres
    environment:
        AIRFLOW__CORE__EXECUTOR: LocalExecutor
        AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
        AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
        AIRFLOW__WEBSERVER__SECRET_KEY: "4a3c818ea3b6c1f6598cbcc61f748abcbe18299c79e452d2d1e7f7310f8e178e" # generate secret key important for airflow
    volumes:
        - ./dags:/opt/airflow/dags
        - ./operators:/opt/airflow/plugins/operators
        - ./utils:/home/root/utils
        - ./data:/mnt/d/Mariusz/nord/data # mounting path needs to be adapted to the one where solution is installed, the same  should be set in download_hosts_operator.py line 16
    ports:
        - "8080:8080"
    command: >
        bash -c "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
        airflow webserver
        "
  # TODO provider apache-airflow-providers-apache-spark is very heavy and image build is quite long operation 
  # TODO to rethink approach of building image, currently build Dockerfile with cache and then use it in composer
  # TODO should help for now

  airflow_scheduler:
    build:
      context: .
      dockerfile: Docker.airflow
    image: my-airflow:2.8.1
    container_name: airflow_scheduler
    user: root
    depends_on:
      - airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__SECRET_KEY: "4a3c818ea3b6c1f6598cbcc61f748abcbe18299c79e452d2d1e7f7310f8e178e" # generate secret key important for airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./operators:/opt/airflow/plugins/operators
      - ./utils:/home/root/utils
      - ./data:/mnt/d/Mariusz/nord/data
    command: >
      bash -c "airflow scheduler"

volumes:
  postgres_data:
