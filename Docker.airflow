FROM apache/airflow:2.8.1-python3.9

USER root

ENV SPARK_VERSION=3.5.6
ENV HADOOP_VERSION=3

RUN apt-get update && apt-get install -y curl default-jdk && \
    curl -fsSL https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz | tar -xz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    chown -R airflow: /opt/spark

ENV JAVA_HOME=/usr/lib/jvm/default-java
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$JAVA_HOME/bin:$PATH

USER airflow

ENV JAVA_HOME=/usr/lib/jvm/default-java
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$JAVA_HOME/bin:$PATH

RUN pip install --no-cache-dir \
    apache-airflow-providers-apache-spark \
    'protobuf>=3.12.0,<5.0.0,!=3.18.*,!=3.19.*' \
    'Flask>=2.0,<3.0'
 
USER root

# TODO add requirements.txt
# COPY requirements.txt .
# RUN pip install --no-cache-dir -r requirements.txt

